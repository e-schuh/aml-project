{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from src.models import models\n",
    "test_model = getattr(models, \"BertForMLM\")(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XmodForMaskedLM were not initialized from the model checkpoint at ZurichNLP/swissbert-xlm-vocab and are newly initialized: ['roberta.encoder.layer.0.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.0.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.0.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.0.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.1.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.1.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.1.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.1.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.10.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.10.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.10.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.10.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.11.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.11.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.11.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.11.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.2.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.2.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.2.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.2.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.3.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.3.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.3.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.3.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.4.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.4.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.4.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.4.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.5.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.5.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.5.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.5.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.6.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.6.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.6.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.6.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.7.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.7.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.7.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.7.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.8.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.8.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.8.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.8.output.adapter_modules.en_XX.dense2.weight', 'roberta.encoder.layer.9.output.adapter_modules.en_XX.dense1.bias', 'roberta.encoder.layer.9.output.adapter_modules.en_XX.dense1.weight', 'roberta.encoder.layer.9.output.adapter_modules.en_XX.dense2.bias', 'roberta.encoder.layer.9.output.adapter_modules.en_XX.dense2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XmodModel were not initialized from the model checkpoint at facebook/xmod-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of XmodModel were not initialized from the model checkpoint at facebook/xmod-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from src.refine_lm import model_BERT\n",
    "\n",
    "test_model_2 = getattr(model_BERT, \"CustomBERTModel\")(8, 10, intrasentence_model = \"SwissBertForMLM\", pretrained_model_name = \"ZurichNLP/swissbert-xlm-vocab\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomBERTModel(\n",
      "  (distilbert): SwissBertForMLM(\n",
      "    (model): XmodForMaskedLM(\n",
      "      (roberta): XmodModel(\n",
      "        (embeddings): XmodEmbeddings(\n",
      "          (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
      "          (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
      "          (token_type_embeddings): Embedding(1, 768)\n",
      "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (encoder): XmodEncoder(\n",
      "          (layer): ModuleList(\n",
      "            (0-11): 12 x XmodLayer(\n",
      "              (attention): XmodAttention(\n",
      "                (self): XmodSelfAttention(\n",
      "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (output): XmodSelfOutput(\n",
      "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "                  (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                  (dropout): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "              )\n",
      "              (intermediate): XmodIntermediate(\n",
      "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "                (intermediate_act_fn): GELUActivation()\n",
      "              )\n",
      "              (output): XmodOutput(\n",
      "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "                (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "                (dropout): Dropout(p=0.1, inplace=False)\n",
      "                (adapter_modules): ModuleDict(\n",
      "                  (de_CH): XmodAdapter(\n",
      "                    (dense1): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (dense2): Linear(in_features=384, out_features=768, bias=True)\n",
      "                    (adapter_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (fr_CH): XmodAdapter(\n",
      "                    (dense1): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (dense2): Linear(in_features=384, out_features=768, bias=True)\n",
      "                    (adapter_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (it_CH): XmodAdapter(\n",
      "                    (dense1): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (dense2): Linear(in_features=384, out_features=768, bias=True)\n",
      "                    (adapter_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (rm_CH): XmodAdapter(\n",
      "                    (dense1): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (dense2): Linear(in_features=384, out_features=768, bias=True)\n",
      "                    (adapter_act_fn): GELUActivation()\n",
      "                  )\n",
      "                  (en_XX): XmodAdapter(\n",
      "                    (dense1): Linear(in_features=768, out_features=384, bias=True)\n",
      "                    (dense2): Linear(in_features=384, out_features=768, bias=True)\n",
      "                    (adapter_act_fn): GELUActivation()\n",
      "                  )\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): XmodLMHead(\n",
      "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "        (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (decoder): Linear(in_features=768, out_features=250002, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=8, out_features=8, bias=True)\n",
      ")\n",
      "Layer: out.weight\n",
      "Parameters: torch.Size([8, 8])\n",
      "\n",
      "Layer: out.bias\n",
      "Parameters: torch.Size([8])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(test_model_2)\n",
    "for name, param in test_model_2.named_parameters():\n",
    "    if 'bert' not in name:\n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Parameters: {param.size()}\")\n",
    "        print() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AML_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
